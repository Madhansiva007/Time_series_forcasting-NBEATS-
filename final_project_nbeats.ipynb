{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tp5ls7gcH_vV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import json\n",
        "import argparse\n",
        "import random\n",
        "from typing import Tuple, Dict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import statsmodels.api as sm\n",
        "\n",
        "\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def safe_mape(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
        "    y_true = np.array(y_true)\n",
        "    y_pred = np.array(y_pred)\n",
        "    mask = y_true != 0\n",
        "    if mask.sum() == 0:\n",
        "        return float('nan')\n",
        "    return float(np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100.0)\n",
        "\n",
        "def mae(y_true, y_pred):\n",
        "    return float(mean_absolute_error(y_true, y_pred))\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    return float(math.sqrt(mean_squared_error(y_true, y_pred)))\n"
      ],
      "id": "tp5ls7gcH_vV"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iGnUYsivH_vX"
      },
      "outputs": [],
      "source": [
        "def generate_synthetic_multivariate(\n",
        "    timesteps: int = 1200,\n",
        "    n_series: int = 3,\n",
        "    noise_scale: float = 0.1,\n",
        "    seed: int = 42\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    rng = np.random.RandomState(seed)\n",
        "    t = np.arange(timesteps)\n",
        "    data = np.zeros((timesteps, n_series))\n",
        "\n",
        "    global_trend = 0.0005 * (t ** 1.2)\n",
        "\n",
        "    for s in range(n_series):\n",
        "        amp1 = 1.0 + 0.3 * s\n",
        "        amp2 = 0.6 + 0.2 * (s % 2)\n",
        "        phase = s * 0.5\n",
        "\n",
        "        season1 = amp1 * np.sin(2 * np.pi * t / 24 + phase)\n",
        "        season2 = amp2 * np.cos(2 * np.pi * t / (24 * 7) + phase * 0.3)\n",
        "        local_trend = (0.0002 * (t ** 1.1)) * (1 + 0.1 * s)\n",
        "        regime = np.where(t > timesteps * 0.6, 0.5 * np.sin(0.002 * (t - timesteps * 0.6)), 0.0)\n",
        "        noise = rng.normal(scale=noise_scale * (1 + 0.2 * s), size=timesteps)\n",
        "\n",
        "        data[:, s] = 2.0 * global_trend + local_trend + season1 + season2 + regime + noise\n",
        "\n",
        "    data = (data - data.mean(axis=0)) / (data.std(axis=0) + 1e-8)\n",
        "    return t, data\n"
      ],
      "id": "iGnUYsivH_vX"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "K6fbGKgSH_vY"
      },
      "outputs": [],
      "source": [
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, data: np.ndarray, lookback: int, horizon: int):\n",
        "        assert data.ndim == 2, \"data must be shape (timesteps, n_series)\"\n",
        "        self.data = data.astype(np.float32)\n",
        "        self.lookback = int(lookback)\n",
        "        self.horizon = int(horizon)\n",
        "        self.T, self.C = self.data.shape\n",
        "\n",
        "    def __len__(self):\n",
        "        return max(0, self.T - self.lookback - self.horizon + 1)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.data[idx: idx + self.lookback]\n",
        "        y = self.data[idx + self.lookback: idx + self.lookback + self.horizon]\n",
        "        return x.T.copy(), y.T.copy()\n"
      ],
      "id": "K6fbGKgSH_vY"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "W_ebeTd5H_vY"
      },
      "outputs": [],
      "source": [
        "class NBeatsBlock(nn.Module):\n",
        "    def __init__(self, input_dim: int, hidden_dim: int, theta_dim: int):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(hidden_dim, theta_dim)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class NBeatsModel(nn.Module):\n",
        "    def __init__(self, input_length: int, horizon: int, n_series: int, hidden_units: int = 256, stacks: int = 2, blocks_per_stack: int = 3, theta_trend: int = 4, theta_season: int = 8, season_period: float = 24.0, device: torch.device = torch.device('cpu')):\n",
        "        super().__init__()\n",
        "        self.input_length = input_length\n",
        "        self.horizon = horizon\n",
        "        self.n_series = n_series\n",
        "        self.device = device\n",
        "        self.theta_trend = int(theta_trend)\n",
        "        self.theta_season = int(theta_season)\n",
        "        self.season_period = float(season_period)\n",
        "        self.theta_per_series = 2 * self.theta_trend + 4 * self.theta_season\n",
        "        self.blocks = nn.ModuleList()\n",
        "        self.num_blocks = stacks * blocks_per_stack\n",
        "        input_dim = n_series * input_length\n",
        "        for _ in range(self.num_blocks):\n",
        "            theta_dim = n_series * self.theta_per_series\n",
        "            self.blocks.append(NBeatsBlock(input_dim, hidden_units, theta_dim))\n",
        "        self.final_fc = nn.Linear(self.num_blocks * n_series * horizon, n_series * horizon)\n",
        "        self.register_buffer('trend_back_v', torch.tensor(np.vander(np.linspace(-1, 0, input_length), N=self.theta_trend, increasing=True), dtype=torch.float32))\n",
        "        self.register_buffer('trend_fore_v', torch.tensor(np.vander(np.linspace(0, 1, horizon), N=self.theta_trend, increasing=True), dtype=torch.float32))\n",
        "        sincos_back = []\n",
        "        sincos_fore = []\n",
        "        b_time = np.arange(input_length)\n",
        "        f_time = np.arange(horizon)\n",
        "        for k in range(1, self.theta_season + 1):\n",
        "            omega = 2 * math.pi * k / self.season_period\n",
        "            sincos_back.append(np.sin(omega * b_time))\n",
        "            sincos_back.append(np.cos(omega * b_time))\n",
        "            sincos_fore.append(np.sin(omega * f_time))\n",
        "            sincos_fore.append(np.cos(omega * f_time))\n",
        "        self.register_buffer('season_back_b', torch.tensor(np.stack(sincos_back, axis=0), dtype=torch.float32))\n",
        "        self.register_buffer('season_fore_b', torch.tensor(np.stack(sincos_fore, axis=0), dtype=torch.float32))\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        batch = x.shape[0]\n",
        "        device = x.device\n",
        "        inp = x.reshape(batch, -1)\n",
        "        forecasts_blocks = []\n",
        "        for block in self.blocks:\n",
        "            theta = block(inp)\n",
        "            theta = theta.view(batch, self.n_series, self.theta_per_series)\n",
        "            per_series_f = []\n",
        "            for s in range(self.n_series):\n",
        "                the_s = theta[:, s, :]\n",
        "                t_len = self.theta_trend\n",
        "                s_len = self.theta_season\n",
        "                idx = 0\n",
        "                trend_theta = the_s[:, idx: idx + 2 * t_len]\n",
        "                idx += 2 * t_len\n",
        "                season_theta = the_s[:, idx: idx + 4 * s_len]\n",
        "                tb = trend_theta[:, :t_len]\n",
        "                tf = trend_theta[:, t_len:]\n",
        "                b_trend = torch.matmul(tb, self.trend_back_v.to(device).T)\n",
        "                f_trend = torch.matmul(tf, self.trend_fore_v.to(device).T)\n",
        "                bs = season_theta[:, :s_len]\n",
        "                bc = season_theta[:, s_len: 2 * s_len]\n",
        "                fs = season_theta[:, 2 * s_len: 3 * s_len]\n",
        "                fc = season_theta[:, 3 * s_len: 4 * s_len]\n",
        "                coeff_back = torch.stack([v for pair in zip(bs.split(1, dim=1), bc.split(1, dim=1)) for v in pair], dim=1).squeeze(-1)\n",
        "                coeff_fore = torch.stack([v for pair in zip(fs.split(1, dim=1), fc.split(1, dim=1)) for v in pair], dim=1).squeeze(-1)\n",
        "                b_seas = torch.matmul(coeff_back, self.season_back_b.to(device))\n",
        "                f_seas = torch.matmul(coeff_fore, self.season_fore_b.to(device))\n",
        "                f = f_trend + f_seas\n",
        "                per_series_f.append(f.unsqueeze(1))\n",
        "            f_block = torch.cat(per_series_f, dim=1)\n",
        "            forecasts_blocks.append(f_block)\n",
        "        concat = torch.cat([fb.reshape(batch, -1) for fb in forecasts_blocks], dim=1)\n",
        "        out = self.final_fc(concat)\n",
        "        out = out.view(batch, self.n_series, self.horizon)\n",
        "        return out\n"
      ],
      "id": "W_ebeTd5H_vY"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "g6roCeldH_vZ"
      },
      "outputs": [],
      "source": [
        "def sarima_forecast(series: np.ndarray, steps: int = 24) -> np.ndarray:\n",
        "    try:\n",
        "        model = sm.tsa.SARIMAX(series, order=(1, 1, 1), seasonal_order=(1, 1, 1, 24),\n",
        "                               enforce_stationarity=False, enforce_invertibility=False)\n",
        "        res = model.fit(disp=False, maxiter=50)\n",
        "        f = res.forecast(steps)\n",
        "        return np.asarray(f)\n",
        "    except Exception:\n",
        "        return np.ones(steps) * series[-1]\n",
        "\n",
        "class LSTMForecast(nn.Module):\n",
        "    def __init__(self, input_len: int, horizon: int, n_series: int, hidden: int = 64, layers: int = 1):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size=n_series, hidden_size=hidden, num_layers=layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden, n_series * horizon)\n",
        "        self.horizon = horizon\n",
        "        self.n_series = n_series\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)\n",
        "        out, _ = self.lstm(x)\n",
        "        last = out[:, -1, :]\n",
        "        out = self.fc(last)\n",
        "        out = out.view(x.size(0), self.n_series, self.horizon)\n",
        "        return out\n"
      ],
      "id": "g6roCeldH_vZ"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qvFhgb-lH_vZ"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, loader, optimizer, criterion, device, clip_grad=None):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for x_batch, y_batch in loader:\n",
        "        x = torch.tensor(x_batch, dtype=torch.float32, device=device)\n",
        "        y = torch.tensor(y_batch, dtype=torch.float32, device=device)\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(x)\n",
        "        loss = criterion(pred, y)\n",
        "        loss.backward()\n",
        "        if clip_grad:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * x.size(0)\n",
        "    return total_loss / len(loader.dataset)\n",
        "\n",
        "def eval_model(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    preds = []\n",
        "    trues = []\n",
        "    with torch.no_grad():\n",
        "        for x_batch, y_batch in loader:\n",
        "            x = torch.tensor(x_batch, dtype=torch.float32, device=device)\n",
        "            y = torch.tensor(y_batch, dtype=torch.float32, device=device)\n",
        "            pred = model(x)\n",
        "            loss = criterion(pred, y)\n",
        "            total_loss += loss.item() * x.size(0)\n",
        "            preds.append(pred.cpu().numpy())\n",
        "            trues.append(y.cpu().numpy())\n",
        "    preds = np.concatenate(preds, axis=0)\n",
        "    trues = np.concatenate(trues, axis=0)\n",
        "    return total_loss / len(loader.dataset), preds, trues\n",
        "\n",
        "def compute_and_print_metrics(preds: np.ndarray, trues: np.ndarray, label: str = \"Model\"):\n",
        "    results = {}\n",
        "    y_true = trues.reshape(-1)\n",
        "    y_pred = preds.reshape(-1)\n",
        "    results['global_mae'] = mae(y_true, y_pred)\n",
        "    results['global_rmse'] = rmse(y_true, y_pred)\n",
        "    results['global_mape'] = safe_mape(y_true, y_pred)\n",
        "    print(f\"\\n=== {label} GLOBAL METRICS ===\")\n",
        "    print(f\"MAE:  {results['global_mae']:.6f}\")\n",
        "    print(f\"RMSE: {results['global_rmse']:.6f}\")\n",
        "    print(f\"MAPE: {results['global_mape']:.4f}%\")\n",
        "    C = preds.shape[1]\n",
        "    per_series = {}\n",
        "    print(f\"\\nPer-series metrics ({C} series):\")\n",
        "    for s in range(C):\n",
        "        yt = trues[:, s, :].reshape(-1)\n",
        "        yp = preds[:, s, :].reshape(-1)\n",
        "        per_series[s] = {\n",
        "            'mae': mae(yt, yp),\n",
        "            'rmse': rmse(yt, yp),\n",
        "            'mape': safe_mape(yt, yp)\n",
        "        }\n",
        "        print(f\" Series {s}: MAE={per_series[s]['mae']:.6f}, RMSE={per_series[s]['rmse']:.6f}, MAPE={per_series[s]['mape']:.4f}%\")\n",
        "    H = preds.shape[2]\n",
        "    per_horizon = {}\n",
        "    print(f\"\\nPer-horizon MAE (first 8 shown):\")\n",
        "    per_h_mae = np.mean(np.abs(preds - trues), axis=(0,1))\n",
        "    per_h_mape = np.mean(np.abs((preds - trues) / (trues + 1e-8)), axis=(0,1)) * 100.0\n",
        "    for h in range(H):\n",
        "        per_horizon[h] = {'mae': float(per_h_mae[h]), 'mape': float(per_h_mape[h])}\n",
        "        if h < 8:\n",
        "            print(f\"  step +{h+1:2d}: MAE={per_h_mae[h]:.6f}, MAPE={per_h_mape[h]:.4f}%\")\n",
        "    results['per_series'] = per_series\n",
        "    results['per_horizon'] = per_horizon\n",
        "    return results\n"
      ],
      "id": "qvFhgb-lH_vZ"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "D0NrfYENH_va"
      },
      "outputs": [],
      "source": [
        "def seasonal_naive_forecast(history: np.ndarray, steps: int, season_period: int = 24) -> np.ndarray:\n",
        "    if len(history) < season_period:\n",
        "        return np.ones(steps) * history[-1]\n",
        "    last_season = history[-season_period:]\n",
        "    reps = int(np.ceil(steps / season_period))\n",
        "    tiled = np.tile(last_season, reps)[:steps]\n",
        "    return tiled\n",
        "\n",
        "def compute_relative_improvement(base_metric: float, model_metric: float) -> float:\n",
        "    if base_metric == 0:\n",
        "        return float('nan')\n",
        "    return (base_metric - model_metric) / base_metric * 100.0\n",
        "\n",
        "def bootstrap_prediction_intervals(preds: np.ndarray, trues: np.ndarray, n_bootstrap: int = 500, alpha: float = 0.05, random_state: int = 42):\n",
        "    rng = np.random.RandomState(random_state)\n",
        "    N, C, H = preds.shape\n",
        "    boot_means = np.zeros((n_bootstrap, C, H), dtype=np.float32)\n",
        "    for b in range(n_bootstrap):\n",
        "        idx = rng.randint(0, N, size=N)\n",
        "        sample_preds = preds[idx]\n",
        "        boot_means[b] = sample_preds.mean(axis=0)\n",
        "    lower = np.percentile(boot_means, 100 * (alpha / 2.0), axis=0)\n",
        "    upper = np.percentile(boot_means, 100 * (1 - alpha / 2.0), axis=0)\n",
        "    return lower, upper\n"
      ],
      "id": "D0NrfYENH_va"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "iOEjCP63H_vb"
      },
      "outputs": [],
      "source": [
        "def run_experiment_notebook(args):\n",
        "    set_seed(args.seed)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.force_cpu else \"cpu\")\n",
        "    os.makedirs(args.artifacts_dir, exist_ok=True)\n",
        "    t, data = generate_synthetic_multivariate(timesteps=args.timesteps, n_series=args.n_series, noise_scale=args.noise, seed=args.seed)\n",
        "    T, C = data.shape\n",
        "    train_cut = int(T * args.train_ratio)\n",
        "    val_cut = int(T * (args.train_ratio + args.val_ratio))\n",
        "    train_data = data[:train_cut]\n",
        "    val_data = data[train_cut:val_cut]\n",
        "    test_data = data[val_cut:]\n",
        "    train_ds = TimeSeriesDataset(np.vstack([train_data, val_data]), args.lookback, args.horizon)\n",
        "    test_ds = TimeSeriesDataset(test_data, args.lookback, args.horizon)\n",
        "    train_loader = DataLoader(train_ds, batch_size=args.batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_ds, batch_size=args.batch_size, shuffle=False)\n",
        "    model = NBeatsModel(input_length=args.lookback, horizon=args.horizon, n_series=C,\n",
        "                        hidden_units=args.hidden, stacks=args.stacks, blocks_per_stack=args.blocks,\n",
        "                        theta_trend=args.theta_trend, theta_season=args.theta_season,\n",
        "                        season_period=args.season_period, device=device).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=3)\n",
        "    criterion = nn.MSELoss()\n",
        "    best_val = float('inf')\n",
        "    best_state = None\n",
        "    wait = 0\n",
        "    for epoch in range(1, args.epochs + 1):\n",
        "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device, clip_grad=args.grad_clip)\n",
        "        val_loss, _, _ = eval_model(model, test_loader, criterion, device)\n",
        "        scheduler.step(val_loss)\n",
        "        print(f\"Epoch {epoch:02d}/{args.epochs} - train_loss={train_loss:.6f} - val_loss={val_loss:.6f} - lr={optimizer.param_groups[0]['lr']:.2e}\")\n",
        "        if val_loss + 1e-9 < best_val:\n",
        "            best_val = val_loss\n",
        "            best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
        "            wait = 0\n",
        "        else:\n",
        "            wait += 1\n",
        "            if wait >= args.patience:\n",
        "                print(\"Early stopping: no improvement.\")\n",
        "                break\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "    _, preds, trues = eval_model(model, test_loader, criterion, device)\n",
        "    preds = preds.reshape(-1, C, args.horizon)\n",
        "    trues = trues.reshape(-1, C, args.horizon)\n",
        "    metrics_nbeats = compute_and_print_metrics(preds, trues, label=\"N-BEATS (final)\")\n",
        "    torch.save(model.state_dict(), os.path.join(args.artifacts_dir, \"nbeats_final.pth\"))\n",
        "    with open(os.path.join(args.artifacts_dir, \"metrics_nbeats.json\"), \"w\") as f:\n",
        "        json.dump(metrics_nbeats, f, indent=2)\n",
        "    preds_flat = preds.reshape(-1, C * args.horizon)\n",
        "    trues_flat = trues.reshape(-1, C * args.horizon)\n",
        "    pd.DataFrame(preds_flat).to_csv(os.path.join(args.artifacts_dir, \"preds_nbeats.csv\"), index=False)\n",
        "    pd.DataFrame(trues_flat).to_csv(os.path.join(args.artifacts_dir, \"trues_nbeats.csv\"), index=False)\n",
        "    lstm = LSTMForecast(args.lookback, args.horizon, C, hidden=args.lstm_hidden, layers=args.lstm_layers).to(device)\n",
        "    opt_l = torch.optim.Adam(lstm.parameters(), lr=args.lr)\n",
        "    for e in range(args.lstm_epochs):\n",
        "        _ = train_one_epoch(lstm, train_loader, opt_l, criterion, device, clip_grad=args.grad_clip)\n",
        "    _, p_l, t_l = eval_model(lstm, test_loader, criterion, device)\n",
        "    p_l = p_l.reshape(-1, C, args.horizon)\n",
        "    t_l = t_l.reshape(-1, C, args.horizon)\n",
        "    metrics_lstm = compute_and_print_metrics(p_l, t_l, label=\"LSTM (baseline)\")\n",
        "    torch.save(lstm.state_dict(), os.path.join(args.artifacts_dir, \"lstm_baseline.pth\"))\n",
        "    with open(os.path.join(args.artifacts_dir, \"metrics_lstm.json\"), \"w\") as f:\n",
        "        json.dump(metrics_lstm, f, indent=2)\n",
        "    sarima_preds = []\n",
        "    sarima_trues = []\n",
        "    for i in range(len(test_data) - args.lookback - args.horizon + 1):\n",
        "        history = np.vstack([train_data, val_data, test_data[:i + args.lookback]])\n",
        "        f = np.zeros((C, args.horizon))\n",
        "        for s in range(C):\n",
        "            f[s, :] = sarima_forecast(history[:, s], steps=args.horizon)\n",
        "        sarima_preds.append(f[np.newaxis, ...])\n",
        "        y_true = test_data[i + args.lookback: i + args.lookback + args.horizon].T\n",
        "        sarima_trues.append(y_true[np.newaxis, ...])\n",
        "    if sarima_preds:\n",
        "        sarima_preds = np.concatenate(sarima_preds, axis=0)\n",
        "        sarima_trues = np.concatenate(sarima_trues, axis=0)\n",
        "        metrics_sarima = compute_and_print_metrics(sarima_preds, sarima_trues, label=\"SARIMA (baseline)\")\n",
        "        with open(os.path.join(args.artifacts_dir, \"metrics_sarima.json\"), \"w\") as f:\n",
        "            json.dump(metrics_sarima, f, indent=2)\n",
        "    # Seasonal naive baseline\n",
        "    seasonal_preds = []\n",
        "    seasonal_trues = []\n",
        "    for i in range(len(test_data) - args.lookback - args.horizon + 1):\n",
        "        history = np.vstack([train_data, val_data, test_data[:i + args.lookback]])\n",
        "        f = np.zeros((C, args.horizon))\n",
        "        for s in range(C):\n",
        "            f[s, :] = seasonal_naive_forecast(history[:, s], steps=args.horizon, season_period=int(args.season_period))\n",
        "        seasonal_preds.append(f[np.newaxis, ...])\n",
        "        y_true = test_data[i + args.lookback: i + args.lookback + args.horizon].T\n",
        "        seasonal_trues.append(y_true[np.newaxis, ...])\n",
        "    if seasonal_preds:\n",
        "        seasonal_preds = np.concatenate(seasonal_preds, axis=0)\n",
        "        seasonal_trues = np.concatenate(seasonal_trues, axis=0)\n",
        "        metrics_seasonal = compute_and_print_metrics(seasonal_preds, seasonal_trues, label=\"Seasonal Naive (baseline)\")\n",
        "        nbeats_mape = metrics_nbeats['global_mape']\n",
        "        naive_mape = metrics_seasonal['global_mape']\n",
        "        mape_impr = compute_relative_improvement(naive_mape, nbeats_mape)\n",
        "        nbeats_mae = metrics_nbeats['global_mae']\n",
        "        naive_mae = metrics_seasonal['global_mae']\n",
        "        mae_impr = compute_relative_improvement(naive_mae, nbeats_mae)\n",
        "        print(f\"\\nRelative improvement over Seasonal Naive:\\n  MAPE improvement: {mape_impr:.2f}%\\n  MAE improvement:  {mae_impr:.2f}%\")\n",
        "    n_boot = args.bootstrap_n if args.bootstrap_n > 0 else 0\n",
        "    if n_boot > 0:\n",
        "        print(f\"\\nComputing bootstrap prediction intervals with {n_boot} samples...\")\n",
        "        lower_ci, upper_ci = bootstrap_prediction_intervals(preds, trues, n_bootstrap=n_boot, alpha=0.05, random_state=args.seed)\n",
        "        for s in range(C):\n",
        "            df_ci = pd.DataFrame({'horizon_step': np.arange(1, args.horizon + 1), 'lower': lower_ci[s, :], 'upper': upper_ci[s, :]})\n",
        "            df_ci.to_csv(os.path.join(args.artifacts_dir, f\"nbeats_bootstrap_ci_series{s}.csv\"), index=False)\n",
        "        print(\"Bootstrap CI CSVs saved to artifacts/\")\n",
        "        s = 0\n",
        "        sample_idx = 0\n",
        "        input_window = test_data[sample_idx: sample_idx + args.lookback, s]\n",
        "        true_future = test_data[sample_idx + args.lookback: sample_idx + args.lookback + args.horizon, s]\n",
        "        pred_mean = preds[sample_idx, s, :]\n",
        "        lower = lower_ci[s, :]\n",
        "        upper = upper_ci[s, :]\n",
        "        timeline = np.arange(sample_idx, sample_idx + args.lookback + args.horizon)\n",
        "        plt.figure(figsize=(10,4))\n",
        "        plt.plot(timeline[:args.lookback], input_window, label='input')\n",
        "        plt.plot(timeline[args.lookback:], true_future, label='true')\n",
        "        plt.plot(timeline[args.lookback:], pred_mean, label='nbeats_pred', lw=2)\n",
        "        plt.fill_between(timeline[args.lookback:], lower, upper, color='gray', alpha=0.25, label='95% CI')\n",
        "        plt.legend()\n",
        "        plt.title('Forecast + 95% CI (series 0)')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(args.artifacts_dir, 'example_forecast_with_ci_series0.png'), dpi=150)\n",
        "        plt.close()\n",
        "    summary = {\n",
        "        'nbeats_metrics': metrics_nbeats,\n",
        "        'lstm_metrics': metrics_lstm,\n",
        "        'artifact_dir': os.path.abspath(args.artifacts_dir)\n",
        "    }\n",
        "    with open(os.path.join(args.artifacts_dir, 'summary.json'), 'w') as f:\n",
        "        json.dump(summary, f, indent=2)\n",
        "    print('\\nAll artifacts saved to:', os.path.abspath(args.artifacts_dir))\n",
        "\n",
        "    return metrics_nbeats\n"
      ],
      "id": "iOEjCP63H_vb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuSzrZ6lH_vc",
        "outputId": "fa99ac07-79a8-463f-b92e-213383ee42bb"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-763811516.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x_batch, dtype=torch.float32, device=device)\n",
            "/tmp/ipython-input-763811516.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  y = torch.tensor(y_batch, dtype=torch.float32, device=device)\n",
            "/tmp/ipython-input-763811516.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x_batch, dtype=torch.float32, device=device)\n",
            "/tmp/ipython-input-763811516.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  y = torch.tensor(y_batch, dtype=torch.float32, device=device)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 01/60 - train_loss=0.200790 - val_loss=0.075901 - lr=1.00e-03\n",
            "Epoch 02/60 - train_loss=0.039698 - val_loss=0.095244 - lr=1.00e-03\n",
            "Epoch 03/60 - train_loss=0.016516 - val_loss=0.017617 - lr=1.00e-03\n",
            "Epoch 04/60 - train_loss=0.007048 - val_loss=0.010835 - lr=1.00e-03\n",
            "Epoch 05/60 - train_loss=0.005071 - val_loss=0.007476 - lr=1.00e-03\n",
            "Epoch 06/60 - train_loss=0.004496 - val_loss=0.006162 - lr=1.00e-03\n",
            "Epoch 07/60 - train_loss=0.004138 - val_loss=0.006318 - lr=1.00e-03\n",
            "Epoch 08/60 - train_loss=0.004156 - val_loss=0.006092 - lr=1.00e-03\n",
            "Epoch 09/60 - train_loss=0.003937 - val_loss=0.008300 - lr=1.00e-03\n",
            "Epoch 10/60 - train_loss=0.003904 - val_loss=0.006142 - lr=1.00e-03\n",
            "Epoch 11/60 - train_loss=0.003864 - val_loss=0.006019 - lr=1.00e-03\n",
            "Epoch 12/60 - train_loss=0.004312 - val_loss=0.006094 - lr=1.00e-03\n",
            "Epoch 13/60 - train_loss=0.004127 - val_loss=0.006923 - lr=1.00e-03\n",
            "Epoch 14/60 - train_loss=0.003950 - val_loss=0.006868 - lr=1.00e-03\n",
            "Epoch 15/60 - train_loss=0.004321 - val_loss=0.007104 - lr=5.00e-04\n",
            "Epoch 16/60 - train_loss=0.003644 - val_loss=0.005720 - lr=5.00e-04\n",
            "Epoch 17/60 - train_loss=0.003525 - val_loss=0.005780 - lr=5.00e-04\n",
            "Epoch 18/60 - train_loss=0.003519 - val_loss=0.005378 - lr=5.00e-04\n",
            "Epoch 19/60 - train_loss=0.003541 - val_loss=0.005336 - lr=5.00e-04\n",
            "Epoch 20/60 - train_loss=0.003417 - val_loss=0.005570 - lr=5.00e-04\n",
            "Epoch 21/60 - train_loss=0.003581 - val_loss=0.005487 - lr=5.00e-04\n",
            "Epoch 22/60 - train_loss=0.003462 - val_loss=0.005343 - lr=5.00e-04\n",
            "Epoch 23/60 - train_loss=0.003452 - val_loss=0.005801 - lr=2.50e-04\n",
            "Epoch 24/60 - train_loss=0.003370 - val_loss=0.005087 - lr=2.50e-04\n",
            "Epoch 25/60 - train_loss=0.003280 - val_loss=0.005078 - lr=2.50e-04\n",
            "Epoch 26/60 - train_loss=0.003252 - val_loss=0.005102 - lr=2.50e-04\n",
            "Epoch 27/60 - train_loss=0.003248 - val_loss=0.005058 - lr=2.50e-04\n",
            "Epoch 28/60 - train_loss=0.003245 - val_loss=0.005256 - lr=2.50e-04\n",
            "Epoch 29/60 - train_loss=0.003256 - val_loss=0.005225 - lr=2.50e-04\n",
            "Epoch 30/60 - train_loss=0.003229 - val_loss=0.005213 - lr=2.50e-04\n",
            "Epoch 31/60 - train_loss=0.003239 - val_loss=0.005326 - lr=1.25e-04\n",
            "Epoch 32/60 - train_loss=0.003163 - val_loss=0.004964 - lr=1.25e-04\n",
            "Epoch 33/60 - train_loss=0.003144 - val_loss=0.004983 - lr=1.25e-04\n",
            "Epoch 34/60 - train_loss=0.003142 - val_loss=0.005002 - lr=1.25e-04\n",
            "Epoch 35/60 - train_loss=0.003137 - val_loss=0.005016 - lr=1.25e-04\n",
            "Epoch 36/60 - train_loss=0.003132 - val_loss=0.005045 - lr=6.25e-05\n",
            "Epoch 37/60 - train_loss=0.003108 - val_loss=0.004890 - lr=6.25e-05\n",
            "Epoch 38/60 - train_loss=0.003100 - val_loss=0.004884 - lr=6.25e-05\n",
            "Epoch 39/60 - train_loss=0.003096 - val_loss=0.004840 - lr=6.25e-05\n",
            "Epoch 40/60 - train_loss=0.003094 - val_loss=0.004937 - lr=6.25e-05\n",
            "Epoch 41/60 - train_loss=0.003094 - val_loss=0.004845 - lr=6.25e-05\n",
            "Epoch 42/60 - train_loss=0.003090 - val_loss=0.004837 - lr=6.25e-05\n",
            "Epoch 43/60 - train_loss=0.003103 - val_loss=0.004987 - lr=6.25e-05\n",
            "Epoch 44/60 - train_loss=0.003095 - val_loss=0.004920 - lr=6.25e-05\n",
            "Epoch 45/60 - train_loss=0.003096 - val_loss=0.004851 - lr=6.25e-05\n",
            "Epoch 46/60 - train_loss=0.003092 - val_loss=0.004864 - lr=3.13e-05\n",
            "Epoch 47/60 - train_loss=0.003078 - val_loss=0.004849 - lr=3.13e-05\n",
            "Epoch 48/60 - train_loss=0.003074 - val_loss=0.004852 - lr=3.13e-05\n",
            "Epoch 49/60 - train_loss=0.003062 - val_loss=0.004881 - lr=3.13e-05\n",
            "Epoch 50/60 - train_loss=0.003062 - val_loss=0.004898 - lr=1.56e-05\n",
            "Early stopping: no improvement.\n",
            "\n",
            "=== N-BEATS (final) GLOBAL METRICS ===\n",
            "MAE:  0.056512\n",
            "RMSE: 0.069983\n",
            "MAPE: 4.2962%\n",
            "\n",
            "Per-series metrics (3 series):\n",
            " Series 0: MAE=0.054132, RMSE=0.065757, MAPE=3.7260%\n",
            " Series 1: MAE=0.051227, RMSE=0.063924, MAPE=3.8628%\n",
            " Series 2: MAE=0.064178, RMSE=0.079263, MAPE=5.2998%\n",
            "\n",
            "Per-horizon MAE (first 8 shown):\n",
            "  step + 1: MAE=0.058568, MAPE=5.3020%\n",
            "  step + 2: MAE=0.053835, MAPE=4.7398%\n",
            "  step + 3: MAE=0.055112, MAPE=4.8820%\n",
            "  step + 4: MAE=0.057968, MAPE=4.8987%\n",
            "  step + 5: MAE=0.060464, MAPE=4.7902%\n",
            "  step + 6: MAE=0.057759, MAPE=4.6395%\n",
            "  step + 7: MAE=0.055996, MAPE=4.3111%\n",
            "  step + 8: MAE=0.053319, MAPE=4.0020%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-763811516.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x_batch, dtype=torch.float32, device=device)\n",
            "/tmp/ipython-input-763811516.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  y = torch.tensor(y_batch, dtype=torch.float32, device=device)\n",
            "/tmp/ipython-input-763811516.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x_batch, dtype=torch.float32, device=device)\n",
            "/tmp/ipython-input-763811516.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  y = torch.tensor(y_batch, dtype=torch.float32, device=device)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== LSTM (baseline) GLOBAL METRICS ===\n",
            "MAE:  0.145507\n",
            "RMSE: 0.177216\n",
            "MAPE: 10.6727%\n",
            "\n",
            "Per-series metrics (3 series):\n",
            " Series 0: MAE=0.133502, RMSE=0.162624, MAPE=8.8281%\n",
            " Series 1: MAE=0.133254, RMSE=0.163303, MAPE=10.0067%\n",
            " Series 2: MAE=0.169763, RMSE=0.202738, MAPE=13.1833%\n",
            "\n",
            "Per-horizon MAE (first 8 shown):\n",
            "  step + 1: MAE=0.106192, MAPE=8.9073%\n",
            "  step + 2: MAE=0.096433, MAPE=8.3370%\n",
            "  step + 3: MAE=0.107479, MAPE=9.2479%\n",
            "  step + 4: MAE=0.116584, MAPE=9.0629%\n",
            "  step + 5: MAE=0.131455, MAPE=9.8756%\n",
            "  step + 6: MAE=0.125630, MAPE=9.2983%\n",
            "  step + 7: MAE=0.141506, MAPE=10.1081%\n",
            "  step + 8: MAE=0.135908, MAPE=9.5705%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
          ]
        }
      ],
      "source": [
        "class Args:\n",
        "    pass\n",
        "\n",
        "def default_args():\n",
        "    a = Args()\n",
        "    a.timesteps = 1200\n",
        "    a.n_series = 3\n",
        "    a.lookback = 72\n",
        "    a.horizon = 24\n",
        "    a.batch_size = 64\n",
        "    a.lr = 1e-3\n",
        "    a.epochs = 60\n",
        "    a.patience = 8\n",
        "    a.seed = 42\n",
        "    a.train_ratio = 0.7\n",
        "    a.val_ratio = 0.15\n",
        "    a.noise = 0.1\n",
        "    a.hidden = 256\n",
        "    a.stacks = 2\n",
        "    a.blocks = 3\n",
        "    a.theta_trend = 4\n",
        "    a.theta_season = 8\n",
        "    a.season_period = 24.0\n",
        "    a.grad_clip = 1.0\n",
        "    a.force_cpu = False\n",
        "    a.artifacts_dir = 'artifacts'\n",
        "    a.weight_decay = 1e-6\n",
        "    a.lstm_hidden = 128\n",
        "    a.lstm_layers = 1\n",
        "    a.lstm_epochs = 20\n",
        "    a.bootstrap_n = 200\n",
        "    return a\n",
        "\n",
        "  if __name__ == '__main__':\n",
        "    args = default_args()\n",
        "    run_experiment_notebook(args)\n"
      ],
      "id": "nuSzrZ6lH_vc"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}